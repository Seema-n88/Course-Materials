{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheet 2: Maximum Likelihood Estimation\n",
    "\n",
    "In this exercise sheet, we will look at various properties of maximum-likelihood estimation, and how to find maximum-likelihood parameters.\n",
    "\n",
    "### ML vs. James Stein Estimator (15 P)\n",
    "\n",
    "Let $X_1,\\dots,X_n \\in \\mathbb{R}^d$ be independent draws from a multivariate Gaussian distribution with mean vector $\\mu$ and covariance matrix $\\Sigma = \\sigma^2 I$. It can be shown that the maximum-likelihood estimator of the mean parameter $\\mu$ is the empirical mean given by:\n",
    "$$\n",
    "\\hat \\mu_\\text{ML} = \\frac1N \\sum_{i=1}^N X_i\n",
    "$$\n",
    "It was once believed that the maximum-likelihood estimator was the most accurate possible (i.e. the one with the smallest Euclidean distance from the true mean). However, it was later demonstrated that the following estimator\n",
    "$$\n",
    "\\hat \\mu_{JS} = \\Big(1-\\frac{(d-2) \\cdot \\sigma^2}{n \\cdot \\|\\mu_\\text{ML}\\|^2}\\Big) \\hat \\mu_\\text{ML}\n",
    "$$\n",
    "(a shrinked version of the maximum-likelihood estimator towards the origin) has actually a smaller distance from the true mean when $d \\geq 3$. This however assumes knowledge of the variance of the distribution for which the mean is estimated. This estimator is called the James-Stein estimator. While the proof is a bit involved, this fact can be easily demonstrated empirically through simulation. This is the object of this exercise.\n",
    "\n",
    "The code below draws ten 50-dimensional points from a normal distribution with mean vector $\\mu = (1,\\dots,1)$ and covariance $\\Sigma = I$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def getdata(seed):\n",
    "\n",
    "    n = 10              # data points\n",
    "    d = 50              # dimensionality of data\n",
    "    m = numpy.ones([d]) # true mean\n",
    "    s = 1.0             # true standard deviation\n",
    "\n",
    "    rstate = numpy.random.mtrand.RandomState(seed)\n",
    "    X = rstate.normal(0,1,[n,d])*s+m\n",
    "    \n",
    "    return X,m,s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function computes the maximum likelihood estimator from a sample of the data assumed to be generated by a Gaussian distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "def ML(X):\n",
    "    X_ml = numpy.mean(X,axis=0)\n",
    "    return X_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Based on the ML estimator function, write a function that receives as input the data $(X_i)_{i=1}^n$ and the (known) variance $\\sigma^2$ of the generating distribution, and computes the James-Stein estimator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.91462639  1.23948138  0.83374761  0.88665586  0.65344909  0.88448107\n",
      "  0.98149717  0.70146071  0.45072473  1.21575437  1.36499062  0.76018241\n",
      "  0.60000665  0.8125837   1.2248957   0.53373545  0.73633746  0.81641188\n",
      "  1.4498917   1.15303948  1.28698031  0.4451644   1.35592865  0.67715367\n",
      "  1.43926955  0.532079    1.10380603  1.06916496  0.25293176  0.7045336\n",
      "  0.96147557  0.84081484  1.01017597  1.16098146  1.00983296  0.7433361\n",
      "  1.43645628  1.04301823  0.30578041  1.11197288  0.80106805  0.79281338\n",
      "  0.8230307   0.573341    1.03353905  0.67921304  1.29873171  0.66972837\n",
      "  0.70860476  0.73915327]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import random\n",
    "def JS(X,s):\n",
    "    n = 10\n",
    "    d = 50    \n",
    "    num = (d-2)*(s*s)\n",
    "    X_ml = ML(X)\n",
    "    \n",
    "    X_sq = numpy.square(X_ml)\n",
    "    \n",
    "    X_sqsum = numpy.sum(X_sq)\n",
    "    den = n*X_sqsum\n",
    "    res1 = num/den\n",
    "    res2 = 1 - res1\n",
    "    m_js = res2*X_ml    \n",
    "    return m_js\n",
    "seed = random.randint(0,99)\n",
    "\n",
    "X,m,s = getdata(seed)\n",
    "result = JS(X,s)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to compute the error of the maximum likelihood estimator and the James-Stein estimator for 100 different samples (where each sample consists of 10 draws generated by the function `getdata` with a different random seed). Here, for reproducibility, we use seeds from 0 to 99. The error should be measured as the Euclidean distance between the true mean vector and the estimated mean vector.\n",
    "\n",
    "* **Compute the maximum-likelihood and James-Stein estimations.**\n",
    "* **Measure the error of these estimations.**\n",
    "* **Build a scatter plot comparing these errors for different samples.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-442-6ef3205def15>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-442-6ef3205def15>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    for x in numpy.nditer(is):\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "mlList = []\n",
    "jsList = []\n",
    "d=50\n",
    "seeds = numpy.random.randint(100,size=100)\n",
    "true_mean_vector = numpy.ones([d])\n",
    "for x in numpy.nditer(is):\n",
    "    X,m,s = getdata(x)\n",
    "    m_ml = ML(X)\n",
    "    m_ml_estimate_vector = numpy.sqrt(numpy.sum((true_mean_vector-m_ml)**2))\n",
    "    #print(m_ml_estimate_vector.shape)\n",
    "    mlList.append([m_ml_estimate_vector])\n",
    "    \n",
    "    m_js = JS(X,s)\n",
    "    m_js_estimate_vector = numpy.sqrt(numpy.sum((true_mean_vector-m_js)**2))\n",
    "    jsList.append([m_js_estimate_vector])\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.xlim(0, 5)\n",
    "plt.ylim(0, 5)\n",
    "plt.xlabel('error for Maximum Likelihood estimator')\n",
    "plt.ylabel('error for James Stein estimator')\n",
    "plt.scatter(mlList,jsList,s=10,alpha=0.5)\n",
    "#plt.figure(figsize=(5,5))\n",
    "#plt.plot(mlList,jsList,'-o')\n",
    "#plt.xscale('log');plt.yscale('log'); plt.xlabel('error for Maximum Likelihood estimator'); plt.ylabel('error for James Stein estimator'); plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of a mixture of exponentials (15 P)\n",
    "\n",
    "We consider the following \"mixture of exponentials\" distribution supported on $\\mathbb{R}^+$, that we use to generate data, but whose parameters $\\alpha$ and $\\beta$ are unknown.\n",
    "\n",
    "$$p(x;\\alpha,\\beta) = 0.5 \\cdot \\big[\\alpha e^{-\\alpha x} + \\beta e^{-\\beta x}\\big]$$\n",
    "\n",
    "A dataset $\\mathcal{D} = x_1,\\dots,x_N$ with $N=200$ has been generated from that distribution. It is given below and plotted as a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D=[ 0.74,  0.20,  0.56,  0.05,  0.67,  0.41,  0.74,  4.63,  0.59,  0.39,\n",
    "    0.71,  0.17,  5.34,  0.33,  0.01,  1.11,  0.60,  0.41,  0.65,  1.97,\n",
    "    0.19,  0.80,  0.04,  0.48,  0.54,  0.59,  0.31,  1.40,  0.63,  0.38,\n",
    "    0.36,  0.02,  0.68,  0.72,  0.84,  0.30,  0.01,  1.37,  0.89,  0.10,\n",
    "    0.21,  0.68,  0.14,  0.10,  0.11,  0.01,  0.09,  0.50,  0.34,  0.30,\n",
    "    1.22, 10.05,  0.19,  0.04,  0.13,  1.53,  2.28,  1.76,  0.03,  0.31,\n",
    "    0.37,  0.50,  0.05,  0.30,  0.53,  0.63,  4.20,  0.86,  0.29,  1.98,\n",
    "    1.27,  0.35,  0.43,  0.35,  0.75,  0.25,  1.15,  1.65,  0.82,  0.37,\n",
    "    2.55,  2.75,  3.06,  0.97,  2.65,  8.97,  0.04,  2.98,  0.36,  0.01,\n",
    "    0.85,  0.90,  0.09,  0.01,  0.82,  2.30,  2.09,  0.29,  0.16,  2.12,\n",
    "    5.28,  0.27,  0.15,  1.02,  0.51,  0.02,  1.72,  1.35,  0.51,  0.27,\n",
    "    1.05,  2.24,  3.93,  0.62,  3.38,  0.56,  0.49,  2.84,  0.27,  0.12,\n",
    "    3.99,  0.16,  0.09,  3.61,  0.54,  0.08,  0.31,  1.38,  0.63,  0.61,\n",
    "    0.21,  0.13,  2.28,  2.61,  4.60,  0.02,  0.34,  0.15,  0.07,  2.44,\n",
    "    0.86,  0.73,  2.01,  0.26,  0.72,  1.56,  0.09,  0.97,  0.24,  0.92,\n",
    "    1.05,  0.71,  1.28,  3.79,  1.32,  0.17,  0.39,  2.82,  0.12,  2.06,\n",
    "    2.04,  0.00,  1.94,  0.27,  0.91,  0.36,  0.92,  5.69,  0.33,  0.69,\n",
    "    1.00,  2.19,  0.01,  0.08,  1.16,  0.31,  0.83,  0.41,  1.27,  0.08,\n",
    "    4.69,  0.65,  0.43,  0.10,  2.92,  0.06,  6.21,  0.90,  0.00,  0.52,\n",
    "    0.65,  0.26,  1.94,  0.37,  0.50,  5.66,  4.24,  0.40,  0.39,  7.89]\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.hist(D,bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this dataset, the log-likelihood function is given by\n",
    "\n",
    "\\begin{align*}\n",
    "\\ell(\\alpha,\\beta) &= \\log \\prod_{i=1}^N p(x_i;\\alpha,\\beta) = \\sum_{i=1}^N \\log (e^{-\\alpha x_i} + \\beta e^{-\\beta x_i}) - \\log (2)\n",
    "\\end{align*}\n",
    "\n",
    "Unfortunately, it is difficult to extract the parameters $\\alpha,\\beta$ analytically by solving directly the equation $\\nabla \\ell = 0$. Instead, we will analyze the function over a grid of parameters $\\alpha$, $\\beta$. We know a priori that parameters $\\alpha$ and $\\beta$ are in the intervals $[0.4,1.0]$ and $[1.5,4.5]$ respectively.\n",
    "\n",
    "* **Build a grid on this limited domain and evaluate log-likelihood at each point of the grid.**\n",
    "* **Plot the log-likelihood function as a contour plot, and superpose the grid to it.**\n",
    "\n",
    "Highest log-likelihood values (i.e. most probable parameters) should appear in red, and lowest values should be plotted in blue. Two adjacent lines of the contour plot should represent a log-likelihood difference of 1.0. In your code, favor numpy array operations over Python loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "D=[ 0.74,  0.20,  0.56,  0.05,  0.67,  0.41,  0.74,  4.63,  0.59,  0.39,\n",
    "    0.71,  0.17,  5.34,  0.33,  0.01,  1.11,  0.60,  0.41,  0.65,  1.97,\n",
    "    0.19,  0.80,  0.04,  0.48,  0.54,  0.59,  0.31,  1.40,  0.63,  0.38,\n",
    "    0.36,  0.02,  0.68,  0.72,  0.84,  0.30,  0.01,  1.37,  0.89,  0.10,\n",
    "    0.21,  0.68,  0.14,  0.10,  0.11,  0.01,  0.09,  0.50,  0.34,  0.30,\n",
    "    1.22, 10.05,  0.19,  0.04,  0.13,  1.53,  2.28,  1.76,  0.03,  0.31,\n",
    "    0.37,  0.50,  0.05,  0.30,  0.53,  0.63,  4.20,  0.86,  0.29,  1.98,\n",
    "    1.27,  0.35,  0.43,  0.35,  0.75,  0.25,  1.15,  1.65,  0.82,  0.37,\n",
    "    2.55,  2.75,  3.06,  0.97,  2.65,  8.97,  0.04,  2.98,  0.36,  0.01,\n",
    "    0.85,  0.90,  0.09,  0.01,  0.82,  2.30,  2.09,  0.29,  0.16,  2.12,\n",
    "    5.28,  0.27,  0.15,  1.02,  0.51,  0.02,  1.72,  1.35,  0.51,  0.27,\n",
    "    1.05,  2.24,  3.93,  0.62,  3.38,  0.56,  0.49,  2.84,  0.27,  0.12,\n",
    "    3.99,  0.16,  0.09,  3.61,  0.54,  0.08,  0.31,  1.38,  0.63,  0.61,\n",
    "    0.21,  0.13,  2.28,  2.61,  4.60,  0.02,  0.34,  0.15,  0.07,  2.44,\n",
    "    0.86,  0.73,  2.01,  0.26,  0.72,  1.56,  0.09,  0.97,  0.24,  0.92,\n",
    "    1.05,  0.71,  1.28,  3.79,  1.32,  0.17,  0.39,  2.82,  0.12,  2.06,\n",
    "    2.04,  0.00,  1.94,  0.27,  0.91,  0.36,  0.92,  5.69,  0.33,  0.69,\n",
    "    1.00,  2.19,  0.01,  0.08,  1.16,  0.31,  0.83,  0.41,  1.27,  0.08,\n",
    "    4.69,  0.65,  0.43,  0.10,  2.92,  0.06,  6.21,  0.90,  0.00,  0.52,\n",
    "    0.65,  0.26,  1.94,  0.37,  0.50,  5.66,  4.24,  0.40,  0.39,  7.89]\n",
    "\n",
    "R = np.arange(0.4,1.0,0.1)\n",
    "Q = np.arange(1.5,4.5,0.1)\n",
    "\n",
    "X,Y = np.meshgrid(R,Q)\n",
    "\n",
    "\n",
    "#plt.plot(X, Y)\n",
    "\n",
    "#R = np.arange(0.4,1.0,0.1)\n",
    "#Q = np.arange(1.5,4.5,0.1)\n",
    "#X,Y = np.meshgrid(R,Q)\n",
    "\n",
    "sample = np.array(D)\n",
    "result = []\n",
    "for k in np.nditer([D]):\n",
    "    S += (np.log(np.exp(-X*k) + np.exp(-Y*k)*Y)) - np.log(2)\n",
    "    \n",
    "result = S\n",
    "CS = plt.contour(X,Y,result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradent-Based Optimization (10 P)\n",
    "\n",
    "As an alternative to computing the log-likelihood for a whole grid, we would like to find the optimal parameters $\\alpha,\\beta$ by gradient-based optimization. The partial derivatives of the log-likelihood function are given by:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\ell(\\alpha,\\beta)}{\\partial \\alpha} &= \\sum_{i=1}^N \\frac{e^{-\\alpha x_i} (1 - \\alpha x_i)}{\\alpha e^{-\\alpha x_i} + \\beta e^{-\\beta x_i}}\\\\\n",
    "\\frac{\\partial \\ell(\\alpha,\\beta)}{\\partial \\beta} &= \\sum_{i=1}^N \\frac{e^{-\\beta x_i} (1 - \\beta x_i)}{\\alpha e^{-\\alpha x_i} + \\beta e^{-\\beta x_i}}\n",
    "\\end{align*}\n",
    "\n",
    "A gradient ascent step of the log-likelihood function takes the form\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\alpha\\\\\n",
    "\\beta\n",
    "\\end{pmatrix} \\leftarrow\n",
    "\\begin{pmatrix}\n",
    "\\alpha\\\\\n",
    "\\beta\n",
    "\\end{pmatrix} + \\gamma \\nabla_{\\alpha,\\beta} \\ell(\\alpha,\\beta)\n",
    "$$\n",
    "\n",
    "where $\\gamma$ is a learning rate to be defined. We start with initial parameters $\\alpha=0.7$ and $\\beta=3.0$.\n",
    "\n",
    "\n",
    "* **Implement the gradient ascent procedure.**\n",
    "* **Run the gradient ascent with parameter $\\gamma = 0.005$.**\n",
    "* **Plot the trajectory of the gradient ascent in superposition to the contour plot of the previous exercise.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D=[ 0.74,  0.20,  0.56,  0.05,  0.67,  0.41,  0.74,  4.63,  0.59,  0.39,\n",
    "    0.71,  0.17,  5.34,  0.33,  0.01,  1.11,  0.60,  0.41,  0.65,  1.97,\n",
    "    0.19,  0.80,  0.04,  0.48,  0.54,  0.59,  0.31,  1.40,  0.63,  0.38,\n",
    "    0.36,  0.02,  0.68,  0.72,  0.84,  0.30,  0.01,  1.37,  0.89,  0.10,\n",
    "    0.21,  0.68,  0.14,  0.10,  0.11,  0.01,  0.09,  0.50,  0.34,  0.30,\n",
    "    1.22, 10.05,  0.19,  0.04,  0.13,  1.53,  2.28,  1.76,  0.03,  0.31,\n",
    "    0.37,  0.50,  0.05,  0.30,  0.53,  0.63,  4.20,  0.86,  0.29,  1.98,\n",
    "    1.27,  0.35,  0.43,  0.35,  0.75,  0.25,  1.15,  1.65,  0.82,  0.37,\n",
    "    2.55,  2.75,  3.06,  0.97,  2.65,  8.97,  0.04,  2.98,  0.36,  0.01,\n",
    "    0.85,  0.90,  0.09,  0.01,  0.82,  2.30,  2.09,  0.29,  0.16,  2.12,\n",
    "    5.28,  0.27,  0.15,  1.02,  0.51,  0.02,  1.72,  1.35,  0.51,  0.27,\n",
    "    1.05,  2.24,  3.93,  0.62,  3.38,  0.56,  0.49,  2.84,  0.27,  0.12,\n",
    "    3.99,  0.16,  0.09,  3.61,  0.54,  0.08,  0.31,  1.38,  0.63,  0.61,\n",
    "    0.21,  0.13,  2.28,  2.61,  4.60,  0.02,  0.34,  0.15,  0.07,  2.44,\n",
    "    0.86,  0.73,  2.01,  0.26,  0.72,  1.56,  0.09,  0.97,  0.24,  0.92,\n",
    "    1.05,  0.71,  1.28,  3.79,  1.32,  0.17,  0.39,  2.82,  0.12,  2.06,\n",
    "    2.04,  0.00,  1.94,  0.27,  0.91,  0.36,  0.92,  5.69,  0.33,  0.69,\n",
    "    1.00,  2.19,  0.01,  0.08,  1.16,  0.31,  0.83,  0.41,  1.27,  0.08,\n",
    "    4.69,  0.65,  0.43,  0.10,  2.92,  0.06,  6.21,  0.90,  0.00,  0.52,\n",
    "    0.65,  0.26,  1.94,  0.37,  0.50,  5.66,  4.24,  0.40,  0.39,  7.89]\n",
    "%matplotlib inline\n",
    "#import solution\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats \n",
    "import pylab\n",
    "\n",
    "#solution.s2b(D)\n",
    "gamma = 0.005\n",
    "alpha = 0.7\n",
    "beta = 3.0\n",
    "a=[]\n",
    "b=[]\n",
    "a.append(alpha)\n",
    "b.append(beta)\n",
    "\n",
    "def gradient_ascent(alpha, beta, D, ep, max_iter):\n",
    "    converged = False\n",
    "    iter = 0\n",
    "    num = len(D) # number of samples\n",
    "    \n",
    "    # Iterate Loop\n",
    "    while not converged:\n",
    "        # for each training sample, compute the gradient learning\n",
    "        update_a = 0\n",
    "        update_b = 0\n",
    "        for i in D:\n",
    "            derq = alpha*np.exp(-alpha*i)+beta*np.exp(-beta*i)\n",
    "            dera = np.exp(-alpha*i)*(1-alpha*i)\n",
    "            derb = np.exp(-beta*i)*(1-beta*i)\n",
    "            update_a += dera/derq\n",
    "            update_b += derb/derq\n",
    " \n",
    "        # update alpha and beta\n",
    "        alpha += gamma*update_a\n",
    "        a.append(alpha)\n",
    "        beta += gamma*update_b\n",
    "        b.append(beta)\n",
    "\n",
    "        # mean squared error\n",
    "        e = np.sqrt(np.square(update_a*gamma)+np.square(update_b*gamma))\n",
    "        \n",
    "        if e <= ep:\n",
    "            print 'Converged, iterations: ', iter, '!!!'\n",
    "            converged = True\n",
    "    \n",
    "        iter += 1  # update iter\n",
    "    \n",
    "        if iter == max_iter:\n",
    "            print 'Max interactions exceeded!'\n",
    "            converged = True\n",
    "\n",
    "    return a,b\n",
    "\n",
    "if __name__ == '__main__':\n",
    " \n",
    "    # call gredient ascent, and get intercept(=theta0) and slope(=theta1)\n",
    "    a, b = gradient_ascent(alpha, beta, D, ep=0.01, max_iter=3000)\n",
    "    \n",
    "    # plot\n",
    "    plt.scatter(a, b)\n",
    "    line, = plt.plot(a, b, '-')\n",
    "    plt.show()\n",
    "    print \"Done!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen, the optimization procedure does not converge in reasonable time and seems to oscillate.\n",
    "\n",
    "* **Explain the problem(s) with this approach. Propose a simple improvement of the optimization technique and apply it.**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The partial derivative is not orthogonal to the real gradient contour. To solve this problem, we should compute the gradient vector of alpha and beta with repect to the log-likelihood function in prevoius work, and shrink the learning rate step by step to approach local minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### REPLACE BY YOUR CODE\n",
    "import solution\n",
    "solution.s2c(D)\n",
    "###"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
